---
title: "Gradient Initialize"
date: 2020-07-18
categories: study
---

# gradient vanishing,exploding 문제를 해결하기위한 노력

## 1.문제
- gradient vanishing,exploding  
backpropa(역전파) 과정에서 입력층에 가까워질 수록 기울기가 점차 작아지거나, 커지는 문제  
따라서 입력층에 가까운 가중치들은 학습이 제대로 이루어지지 않아 최적의 모델을 찾을 수 없다.  
심층 신경망 훈련에 어려움  

## 2.원인
- **sigmoid 활성화 함수**  
- **평균이 0, 표준편차가 1인 정규분포에서 가중치 초기화**  

output의 분산이 input의 분산보다 커짐  
신경망이 깊어질수록 분산이 계속 커져 활성화 함수가 0 또는 1로 수렴하게됨.  

sigmoid 함수의 입력이 음수,양수 방향으로 매우 커지게되면 0,1에 수렴하고  
그 지점에서 기울기가 0이되서, 역전파 과정에서 전파될 gradient가 약해짐.  

- **초기 가중치를 모두 0으로 설정**  
데이터의 평균을 0으로 정규화시키고, 가중치를 0으로 초기화하는 것은 합리적으로 보일 수 있지만, 실제로 가중치를 모두 0으로 초기화한다면 뉴런들이 모두 같은 값을 나타낼 것임.  
*각 가중치의 update가 동일하게 이루어짐*  

## 3.초기화를 잘해보자!

  input의 분산과 ouput의 분산이 같아야함  
  그리고 역방향에서 층을 통과하기 전,후의 gradient의 분산이 같아야함.  
- **Xavier initailizaion**  


  *(Relu 활성화 함수를 사용할 때, 출력값이 0에 수렴하는 경우가 있기 때문에 비효율적임)*  
- **He initailizaion**
